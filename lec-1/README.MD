## Лек 1. Задачи из раздела "Элементы теории вероятностей" учебного пособия.

В этом случае мы рассматриваем нашу случайную величину как "вероятность" и по этой "вероятности" восстанавливаем соответствующий ей аргумент интересующей нас функции распределения.

### 1. Функция распределения

В нашем случае мы имеем:

$$
P(F^{-1}(x) < t) = P(x < F(t)) = F(t),
$$

поскольку $x \sim U[0,1]$.

### 2. Выражение случайной величины через нормальное распределение

Можем выразить $\xi$ как:

$$
\xi = \frac{\eta - \mu}{\sigma}
$$

Обозначим плотность нормального распределения с параметрами $\mu$ и $\sigma$ как:

$$
\phi_{\mu, \sigma}(t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(t - \mu)^2}{2\sigma^2}\right)
$$

Тогда:

$$
\phi_{\mu, \sigma}\left(\frac{t + \mu}{\sigma}\right) = \frac{1}{\sigma} \phi_{\mu, \sigma}(\sigma t + \mu) = \frac{1}{\sigma} \cdot \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\sigma t + \mu - \mu)^2}{2\sigma^2}\right) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{t^2}{2}\right) = \phi_{0,1}(t)
$$

### 3. Линейность матожидания и дисперсии

Используем линейность математического ожидания и дисперсии для независимых величин. Для $\xi_i$ имеем $M\xi_i = \frac{1}{2}$ и $D\xi_i = \frac{1}{12}$. Отсюда имеем:

$$
M S_n = M \left(\sum_{i=1}^n \xi_i \right) = \sum_{i=1}^n M\xi_i = \sum_{i=1}^n \frac{1}{2} = \frac{n}{2}
$$

Аналогично для дисперсии:

$$
D S_n = D \left(\sum_{i=1}^n \xi_i \right) = \sum_{i=1}^n D\xi_i = \sum_{i=1}^n \frac{1}{12} = \frac{n}{12}
$$

### 4. Преобразование плотности

Плотность $p_\zeta(t)$ можно выразить через $p_\xi(x)$ как:

$$
p_\zeta(t) = p_\xi(g(t)) |g'(t)|
$$

Таким образом, можно получить что-то неожиданное в случае нелинейного преобразования. Например, если мы рассмотрим равномерное распределение на $[0, 1]$ и $g(t) = e^{-t}$, то в одном случае математическое ожидание будет $\frac{1}{2}$, а в другом — $1$. Либо можно взять арктангенс и равномерное распределение на отрезке от $0$ до $\pi$. В этом случае получится распределение Коши, у которого математическое ожидание не существует.

### 5. Пример преобразования

Пример преобразования, для которого это не выполнится:

Рассмотрим непрерывное равномерное распределение на $[0,1]$. Пусть:

$$
z(x) = \begin{cases} 1, & x > \frac{1}{3} \\ 0, & \text{иначе} \end{cases}
$$

Можно также "отразить" распределение относительно его медианы или математического ожидания каким-нибудь непрерывным преобразованием. Например: $f(x) = |M(\xi) - x|$ или $f(x) = |\text{Median}(\xi) - x|$. Если функция монотонна, то для медианы равенство будет выполняться, но для математического ожидания уже не всегда.

### 6. Ковариационная матрица

Очевидно, в этом случае все недиагональные элементы ковариационной матрицы зануляются, её определитель будет равен произведению элементов диагонали, а на диагонали находятся дисперсии $i$-ых компонент. Аналогично и в показателе экспоненты останутся только скалярные произведения, каждое из которых будет разделено на соответствующую дисперсию. Поскольку в показателе экспоненты будет сумма, это можно расписать в произведение нескольких таких экспонент:

$$
p(x) = \frac{1}{(2\pi)^{m/2} \sqrt{\text{det}\Sigma}} \exp\left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right)
$$

Причем, в силу независимости компонент $x_i$ следует $\Sigma = \text{diag}(\sigma_i^2)$:

$$
\text{det}\Sigma = \prod_{i=1}^n \sigma_i^2
$$

$$(x - \mu)^T \Sigma^{-1} (x - \mu) = \sum_{i=1}^n \frac{(x_i - \mu_i)^2}{\sigma_i^2}$$

Тогда плотность записывается как:

$$
p(x) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left( -\frac{(x_i - \mu_i)^2}{2\sigma_i^2} \right) = \prod_{i=1}^n p_i(x)
$$

### 7. Независимость случайных величин

Пусть $\xi_i$ и $\xi_j$ независимы. Поскольку они независимы, верно:

$$M(\xi_i \xi_j) = M(\xi_i) M(\xi_j)$$

По определению:

$$
\sigma_{ij} = M((\xi_i - M\xi_i)(\xi_j - M\xi_j)) = M(\xi_i \xi_j) - M(\xi_i) M(\xi_j) = M(\xi_i) M(\xi_j) - M(\xi_i) M(\xi_j) = 0
$$

### 8. Неотрицательная определенность матрицы

Матрица $A$ неотрицательно определена, если $u^T A u \geq 0 \, orall u \in \mathbb{R}^n$. В случае, если рассматривается матрица ковариаций случайного вектора $\Sigma = M(XX^T)$, имеем:

$$
u^T \Sigma 
u = 
u^T M(XX^T) 
u = M(
u^T X X^T 
u) = M((
u^T X)^2) \geq 0$$

### 9. Формула полной вероятности

Очевидно, учитывая, что используется формула полной вероятности для несовместных событий, вероятности которых в сумме дают $1$.

