## Лек 1. Задачи из раздела "Элементы теории вероятностей" учебного пособия.

### 1. Задание

> Утверждение 1: Пусть случайная величина $X$ равномерно распределена на $(0, 1)$, а $F(x)$ — функция распределения. Тогда случайная величина $Y = F^{-1}(X)$ распределена в соответствии с $F$.


Пусть $X$ — это случайная величина, равномерно распределенная на отрезке $0$ до $1$. То есть все значения в этом интервале одинаково вероятны. Функция $F(x)$ показывает, насколько вероятно, что случайная величина <= значению $x$. $Y$ получаем, применяя к $X$ обратную функцию $F^{-1}(x)$. Эта функция по заданной вероятности "возвращает" значение случайной величины.

Когда используем обратную функцию $F^{-1}(X)$, мы берем эту вероятность $X$ и преобразуем её обратно в значение случайной величины, следующее распределению $F$. 

Если $X$ равномерно покрывает весь интервал от $0$ до $1$, то при использовании $F^{-1}(X)$ в итоге получаем значения, которые распределены именно так, как определяет функция $F$.
Таким образом, $Y$ будет случайной величиной, распределенной в точности по распределению $F$.


### 2. Задание

> Если у нас есть случайная величина $\epsilon$, которая распределена нормально с параметрами $N(0, 1)$ (стандартное нормальное распределение со средним 0 и дисперсией 1), то можем получить случайную величину $\nu$, которая распределена нормально с параметрами $\mu$ (мат. ожидание) и $\sigma$ (std) по формуле:

$$\nu = \mu + \sigma \epsilon$$ 
- линейное преобразование случайной величины $\epsilon$.
- $\epsilon \sim N(0, 1)$ == $\epsilon$ имеет среднее $0$ и дисперсию $1$.

Поскольку $\epsilon$ распределена нормально, а $\nu$ — это линейное преобразование $\epsilon$, то $\nu$ также будет распределена нормально, с математическим ожиданием $\mu$ и дисперсией $\sigma^2$.

Таким образом, случайная величина $\nu = \mu + \sigma \epsilon$ имеет нормальное распределение с математическим ожиданием $\mu$ и стандартным отклонением $\sigma$, что доказывает утверждение.

### 3. Задание

Пусть $\xi_i$ — независимые случайные величины, равномерно распределенные на $[0, 1]$, и $S_n = \sum_{i=1}^n \xi_i$.

1) Мат. ожидание
- Для каждой $\xi_i$, $M(\xi_i) = \frac{1}{2}$, так как среднее значение равномерного распределения на $[0, 1]$ равно $\frac{1}{2}$.
- Тогда $M(S_n) = M\left(\sum_{i=1}^n \xi_i\right) = \sum_{i=1}^n M(\xi_i) = \sum_{i=1}^n \frac{1}{2} = \frac{n}{2}$.

2. Дисперсия:
- Дисперсия для $\xi_i$ равна $D(\xi_i) = \frac{1}{12}$, так как дисперсия равномерного распределения на $[0, 1]$ равна $\frac{1}{12}$.
- Так как все $\xi_i$ независимы, $D(S_n) = D\left(\sum_{i=1}^n \xi_i\right) = \sum_{i=1}^n D(\xi_i) = \sum_{i=1}^n \frac{1}{12} = \frac{n}{12}$.

Получили, что $M(S_n) = \frac{n}{2}$ и $D(S_n) = \frac{n}{12}$.

### 4. Задание 

Пусть дана случайная величина $\xi$ с функцией плотности распределения $p(x)$. Мы производим замену координат: $x = g(t)$, 
- где $g(t)$ —  функция, которая преобразует $t$ в $x$.

1) Выясним, всегда ли для новой случайной величины $\zeta = g^{-1}(\xi)$ плотность распределения $\zeta$ будет удовлетворять следующему соотношению для математического ожидания: $M(\xi) = M(\zeta)$.

**Функция плотности** распределения случайной величины $\zeta$ определяется через плотность $\xi$ с использованием `правила преобразования плотности`:

$$
p_\zeta(t) = p_\xi(g(t)) |g'(t)|
$$

- Плотность $\zeta$ зависит от функции преобразования $g(t)$ и ее производной. 
- В случае нелинейного преобразования эта `производная может существенно изменять вид плотности распределения`, что влияет на значение мат. ожидания.

2) Теперь рассмотрим точки $x_0$ и $t_0$, в которых `достигается максимум плотности распределения` случайных величин $\xi$ и $\zeta$. 
- Для этого выясним, как функция преобразования $g(t)$ влияет на положение и величину экстремумов плотности:
    - Если $g(t)$ **монотонна**, то экстремумы плотности $\xi$ и $\zeta$ будут $x_0 = g(t_0)$ ==> максимум плотности $\xi$ будет переноситься на максимум плотности $\zeta$.
    - Если функция $g(t)$ **не является монотонной**, максимумы могут сдвинуться или измениться по количеству, что приведет к нарушению условия $M(\xi) = M(\zeta)$.

Таким образом, **утверждение верно**, если функция преобразования `$g(t)$ является монотонной`. В противном случае, если функция `$g(t)$ не монотонна`, то это **равенство может нарушаться**.


### 5. Задание

Eсть случайная величина $\xi$ и функция преобразования $z(\xi)$, которая преобразует $\xi$ в другую случайную величину $\zeta = z(\xi)$. Нужно проверить, будет ли мат. ожидание $M(\zeta)$ всегда равно $z(M(\xi))$.

После преобразования $\xi$ в $\zeta = z(\xi)$ значение мат. ожидания $\zeta$ зависит не только от среднего значения $\xi$, но и от того, `как функция $z(\xi)$ изменяет значения случайной величины`.

- Равенство $M(\zeta) = z(M(\xi))$ **выполняется**, если функция $z(\xi)$ `является линейной`. Cреднее значение $\xi$ просто проходит через функцию $z$.
- Если функция $z(\xi)$ **нелинейна** (например, $z(\xi) = \xi^2$), то мат. ожидание $\zeta$ будет `зависеть от всех значений $\xi$`, а не только от среднего значения. Равенство $M(\zeta) = z(M(\xi))$ `не выполняется`.

- Для **медианы**: если функция $z(\xi)$ **монотонна**, то медиана преобразованной случайной величины $\zeta$ будет `равна преобразованию медианы $\xi$.`
- Но если функция $z(\xi)$ **не является монотонной**, медиана может `изменяться непредсказуемо`, равенства не будет.


### 6. Задание
Используем понятие независимости и свойство плотности совместного распределения для независимых случайных величин.

1) Случайные величины или признаки **независимы**, если `их совместная вероятность` может быть представлена `как произведение вероятностей каждой величины по отдельности`.

Если все компоненты вектора $x = (x_1, x_2, \ldots, x_n)$ независимы, то плотность распределения для вектора можно выразить через плотности распределения его компонентов.


2) Пусть $x_i$ — это компоненты вектора $x$, и каждая $x_i$ распределена нормально с плотностью $p_i(x_i)$. Так как все $x_i$ - независимы, совместная плотность $p(x)$ равна произведению плотностей каждого компонента:

   $$
   p(x) = p_1(x_1) \cdot p_2(x_2) \cdot \ldots \cdot p_n(x_n) = \prod_{i=1}^n p_i(x_i)
   $$

3) Поскольку каждая компонента $x_i$ распределена нормально, то плотность $p_i(x_i)$ для каждой компоненты имеет вид нормального распределения.
   - Совместная плотность $p(x)$ также равна произведению плотностей нормальных распределений для каждого компонента.

Получаем, что если все признаки (компоненты $x$) независимы, то совместная плотность $p(x)$ равна произведению одномерных плотностей компонентов $x$.

### 7. Задание

По заданию имеем, что все признаки $\xi_i$ и $\xi_j$ - независимы. Из этого условия следует, что:

$$M(\xi_i \xi_j) = M(\xi_i) M(\xi_j)$$

По определению следует:

$$
\sigma_{ij} = M((\xi_i - M\xi_i)(\xi_j - M\xi_j)) = M(\xi_i) M(\xi_j) - M(\xi_i) M(\xi_j) = 0
$$

Получили, что $\sigma_{ij} = 0$

### 8. Задание

### Доказательство, что $\Sigma$ — неотрицательно определённая матрица

Имеем ковариационную матрицу $\Sigma$. Чтобы показать, что $\Sigma$ является неотрицательно определённой, нужно доказать, что для любого вектора $u$ верно:

$$
u^T \Sigma u \geq 0
$$

Для матрицы $\Sigma = M(XX^T)$, где $X$ — случайный вектор, выражение $u^T \Sigma u$ можно записать как:

$$
u^T \Sigma u = u^T M(XX^T) u = M((u^T X)^2) \geq 0
$$

Так как квадрат любой случайной величины всегда неотрицателен, выражение $M((u^T X)^2)$ также будет неотрицательным => $\Sigma$ — неотрицательно определённая матрица.

- Матрица $\Sigma$ **не является положительно определённой**, если `существуют линейно зависимые компоненты случайного вектора $X$`. В этом случае хотя бы одно собственное значение матрицы $\Sigma$ равно нулю, и тогда $u^T \Sigma u = 0$ для некоторых ненулевых $u$. 

Как вывод, матрица не строго положительна, а лишь неотрицательно определена.



### 9. Задание

Формула Байеса выражает апостериорную вероятность гипотезы $H_i$ при наличии данных $D$:

$$
P(H_i | D) = \frac{P(D | H_i) P(H_i)}{P(D)}
$$

- $P(D) = \sum_j P(D | H_j) P(H_j)$ — это общая вероятность данных $D$.


Рассмотрим сумму всех апостериорных вероятностей для всех гипотез $H_j$:

$$
\sum_i P(H_i | D) = \sum_i \frac{P(D | H_i) P(H_i)}{P(D)}
$$

При подстановке $P(D)$ в знаменатель, получаем:

$$
\sum_i P(H_i | D) = \frac{\sum_i P(D | H_i) P(H_i)}{P(D)} = \frac{P(D)}{P(D)} = 1
$$

Получили, что сумма апостериорных вероятностей всех гипотез равна $1$.

